Kernel Driver ibmvmc
====================

Authors:
	Dave Engebretsen <engebret@us.ibm.com>
	Adam Reznechek <adreznec@linux.vnet.ibm.com>
	Steven Royer <seroyer@linux.vnet.ibm.com>
	Bryant G. Ly <bryantly@linux.vnet.ibm.com>

Description
===========

The Virtual Management Channel (VMC) is a logical device which provides an
interface between the hypervisor and a management partition. This management
partition is intended to provide an alternative to HMC-based system management.
In the management partition, a Logical Partition Manager (LPM) application
exists which enables a system administrator to configure the system’s
partitioning characteristics via a command line interface or web browser.
Support for conventional HMC management of the system may still be provided on
a system; however, when an HMC is attached to the system, the VMC interface is
disabled by the hypervisor.

Logical Partition Manager (LPM)
The LPM is a browser based LPAR configuration tool provided by the management
partition. System configuration, maintenance, and control functions which
traditionally require an HMC can be implemented in the LPM using a combination
of HMC to hypervisor interfaces and existing operating system methods. This tool
provides a subset of the functions implemented by the HMC and enables basic
partition configuration. The set of HMC to hypervisor messages supported by the
LPM component are passed to the hypervisor over a VMC interface, which is
defined below.

Virtual Management Channel (VMC)
A logical device, called the virtual management channel (VMC), is defined for
communicating between the LPM application and the hypervisor. This device,
similar to a VSCSI server device, is presented to a designated management
partition as a virtual device and is only presented when the system is not HMC
managed.
This communication device borrows aspects from both VSCSI and ILLAN devices and
is implemented using the CRQ and the RDMA interfaces. A three-way handshake is
defined that must take place to establish that both the hypervisor and
management partition sides of the channel are running prior to sending/receiving
any of the protocol messages.
This driver also utilizes Transport Event CRQs. CRQ messages that are sent when
the hypervisor detects one of the peer partitions has abnormally terminated, or
one side has called H_FREE_CRQ to close their CRQ.
Two new classes of CRQ messages are introduced for the VMC device. VMC
Administrative messages are used for each partition using the VMC to communicate
capabilities to their partner. HMC Interface messages are used for the actual
flow of HMC messages between the management partition and the hypervisor.
As most HMC messages far exceed the size of a CRQ bugger, a virtual DMA (RMDA)
of the HMC message data is done prior to each HMC Interface CRQ message.
Only the management partition drives RDMA operations; hypervisors never directly
causes the movement of message data.

Example Management Partition VMC Driver Interface
=================================================
This section provides an example for the LPM implementation where a device
driver is used to interface to the VMC device. This driver consists of a new
device, for example /dev/lparvmc, which provides interfaces to open, close,
read, write, and perform ioctl’s against the VMC device.

VMC Interface Initialization
The device driver is responsible for initializing the VMC when the driver is
loaded. It first creates and initializes the CRQ. Next, an exchange of VMC
capabilities is performed to indicate the code version and number of resources
available in both the management partition and the hypervisor. Finally, the
hypervisor requests that the management partition create an initial pool of VMC
buffers, one buffer for each possible HMC connection, which will be used for LPM
session initialization. Prior to completion of this initialization sequence, the
device returns EBUSY to open() calls. EIO is returned for all open() failures.

Management Partition		Hypervisor
		CRQ INIT
---------------------------------------->
	   CRQ INIT COMPLETE
<----------------------------------------
	      CAPABILITIES
---------------------------------------->
	 CAPABILITIES RESPONSE
<----------------------------------------
      ADD BUFFER (MHC IDX=0,1,..)	  _
<----------------------------------------  |
	  ADD BUFFER RESPONSE		   | - Perform # HMCs Iterations
----------------------------------------> -

VMC Interface Open
After the basic VMC channel has been initialized, an HMC session level
connection can be established. The application layer performs an open() to the
VMC device and executes an ioctl() against it, indicating the HMC ID (32 bytes
of data) for this session. If the VMC device is in an invalid state, EIO will be
returned for the ioctl(). The device driver creates a new HMC session value
(ranging from 1 to 255) and HMC index value (starting at index 0 and potentially
ranging to 254 in future releases) for this HMC ID. The driver then does an RDMA
of the HMC ID to the hypervisor, and then sends an Interface Open message to the
hypervisor to establish the session over the VMC. After the hypervisor receives
this information, it sends Add Buffer messages to the management partition to
seed an initial pool of buffers for the new HMC connection. Finally, the
hypervisor sends an Interface Open Response message, to indicate that it is
ready for normal runtime messaging. The following illustrates this VMC flow:

Management Partition             Hypervisor
	      RDMA HMC ID
---------------------------------------->
	    Interface Open
---------------------------------------->
	      Add Buffer		  _
<----------------------------------------  |
	  Add Buffer Response		   | - Perform N Iterations
----------------------------------------> -
	Interface Open Response
<----------------------------------------

VMC Interface Runtime
During normal runtime, the LPM application and the hypervisor exchange HMC
messages via the Signal VMC message and RDMA operations. When sending data to
the hypervisor, the LPM application performs a write() to the VMC device, and
the driver RDMA’s the data to the hypervisor and then sends a Signal Message. If
a write() is attempted before VMC device buffers have been made available by the
hypervisor, or no buffers are currently available, EBUSY is returned in response
to the write(). A write() will return EIO for all other errors, such as an
invalid device state. When the hypervisor sends a message to the LPM, the data
is put into a VMC buffer and an Signal Message is sent to the VMC driver in the
management partition. The driver RDMA’s the buffer into the partition and passes
the data up to the appropriate LPM application via a read() to the VMC device.
The read() request blocks if there is no buffer available to read. The LPM
application may use select() to wait for the VMC device to become ready with
data to read.

Management Partition             Hypervisor
		MSG RDMA
---------------------------------------->
		SIGNAL MSG
---------------------------------------->
		SIGNAL MSG
<----------------------------------------
		MSG RDMA
<----------------------------------------

VMC Interface Close
HMC session level connections are closed by the management partition when the
application layer performs a close() against the device. This action results in
an Interface Close message flowing to the hypervisor, which causes the session
to be terminated. The device driver must free any storage allocated for buffers
for this HMC connection.

Management Partition             Hypervisor
	     INTERFACE CLOSE
---------------------------------------->
        INTERFACE CLOSE RESPONSE
<----------------------------------------

For more information on the documentation for CRQ Messages, VMC Messages,
HMC interface Buffers, and signal messages please refer to the Linux on Power
Architecture Platform Reference. Section F.
